{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNRQ9inpXOJE/nineJ29GqL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import os\n","import pandas as pd\n","from datetime import datetime, timedelta\n","\n","# Step 1: Define the base directory where the data is stored\n","base_dir = \"/content/MinuteRaw15/MinuteRaw15\"  # Update to the exact path where year folders are located\n","\n","# Step 2: Initialize an empty DataFrame to combine all data\n","combined_df = pd.DataFrame()\n","\n","# Step 3: Loop through each year folder\n","for year in range(2012, 2024):\n","    year_dir = os.path.join(base_dir, str(year))  # Path to the year folder\n","    if not os.path.exists(year_dir):  # Check if the directory exists\n","        print(f\"Year directory not found: {year_dir}\")\n","        continue\n","\n","    # Get the sorted list of .csv files for each year\n","    csv_files = sorted([f for f in os.listdir(year_dir) if f.endswith('.csv')])\n","\n","    for file in csv_files:\n","        file_path = os.path.join(year_dir, file)\n","        monthly_df = pd.read_csv(file_path)  # Read each CSV file\n","        combined_df = pd.concat([combined_df, monthly_df], ignore_index=True)  # Concatenate the data\n","\n","# Step 4: Create the 'date' column\n","start_time = datetime(2012, 1, 1, 0, 15)\n","time_intervals = [start_time + timedelta(minutes=15 * i) for i in range(len(combined_df))]\n","combined_df['date'] = time_intervals\n","\n","# Step 5: Save the combined dataset\n","output_file = \"/content/combined_time_series.csv\"\n","combined_df.to_csv(output_file, index=False)\n","\n","print(f\"Combined dataset saved to {output_file}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BLCnNegwhB7J","executionInfo":{"status":"ok","timestamp":1732910610750,"user_tz":360,"elapsed":9297,"user":{"displayName":"kushal bhalla","userId":"14624985245222236490"}},"outputId":"c71ab12d-7cc9-4324-fe83-2af6b1179b6d"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Combined dataset saved to /content/combined_time_series.csv\n"]}]},{"cell_type":"code","source":["12*365*24*4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SULjSd_2hon5","executionInfo":{"status":"ok","timestamp":1732910693865,"user_tz":360,"elapsed":92,"user":{"displayName":"kushal bhalla","userId":"14624985245222236490"}},"outputId":"91aaee7f-40ce-4e68-9514-55bf27db2a9c"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["420480"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["from google.colab import files\n","import os\n","import pandas as pd\n","from datetime import datetime, timedelta\n","import zipfile\n","\n","# Step 1: Upload and unzip the folder\n","uploaded = files.upload()\n","\n","zip_file = \"/content/MinuteRaw15.zip\"\n","extract_dir = \"/content/MinuteRaw15\"\n","with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n","    zip_ref.extractall(extract_dir)\n","\n","# Step 2: Dynamically find the correct base directory\n","for root, dirs, files in os.walk(\"/content\"):\n","    if \"2012\" in dirs:\n","        base_dir = os.path.join(root, \"MinuteRaw15\", \"MinuteRaw15\")  # Adjusted to match the actual path\n","        print(f\"Base directory set to: {base_dir}\")\n","        break\n","\n","\n","# Step 3: Combine files\n","combined_df = pd.DataFrame()\n","\n","for year in range(2012, 2024):\n","    year_dir = os.path.join(base_dir, str(year))  # Path to the year folder\n","    if not os.path.exists(year_dir):  # Check if the directory exists\n","        print(f\"Year directory not found: {year_dir}\")\n","        continue\n","\n","    csv_files = sorted([f for f in os.listdir(year_dir) if f.endswith('.csv')])\n","\n","    for file in csv_files:\n","        file_path = os.path.join(year_dir, file)\n","        monthly_df = pd.read_csv(file_path)\n","        combined_df = pd.concat([combined_df, monthly_df], ignore_index=True)\n","\n","# Step 4: Create the 'date' column\n","start_time = datetime(2012, 1, 1, 0, 15)\n","time_intervals = [start_time + timedelta(minutes=15 * i) for i in range(len(combined_df))]\n","combined_df['date'] = time_intervals\n","\n","# Step 5: Save the combined dataset\n","output_file = \"/content/combined_time_series.csv\"\n","combined_df.to_csv(output_file, index=False)\n","\n","print(f\"Combined dataset saved to {output_file}\")\n"],"metadata":{"id":"jqrMtjs_imyC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# IN ORDER TO CHECK THE TIME PROGRESSION\n","\n","import pandas as pd\n","\n","# Load the combined dataset\n","file_path = \"/content/combined_time_series.csv\"\n","df = pd.read_csv(file_path)\n","\n","# Check the start and end of the dataset\n","print(\"Start date:\", df['date'].iloc[0])\n","print(\"End date:\", df['date'].iloc[-1])\n","\n","# Verify that intervals are consistent\n","df['date'] = pd.to_datetime(df['date'])\n","time_differences = df['date'].diff().dropna().value_counts()\n","\n","print(\"Unique time intervals in the dataset:\", time_differences)\n"],"metadata":{"id":"uEt9w2j4kikS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# CHECKING THE MONTHLY CONCATENATION\n","\n","# Extract year and month from the 'date' column\n","df['year'] = df['date'].dt.year\n","df['month'] = df['date'].dt.month\n","\n","# Group by year and month and count rows\n","month_counts = df.groupby(['year', 'month']).size().reset_index(name='row_count')\n","\n","# Display the first few rows and the total counts\n","print(month_counts.head(12))  # Display data for 2012\n","print(month_counts.tail(12))  # Display data for 2023\n"],"metadata":{"id":"NYvQjcU3ksMg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# CHECKING DATA FROM SPECIFIC MONTHS\n","\n","# Example: Check January 2012\n","jan_12 = df[df['date'].dt.to_period('M') == '2012-01']\n","print(\"January 2012 rows:\", len(jan_12))\n","print(\"First timestamp in January 2012:\", jan_12['date'].iloc[0])\n","print(\"Last timestamp in January 2012:\", jan_12['date'].iloc[-1])\n","\n","# Example: Check December 2023\n","dec_23 = df[df['date'].dt.to_period('M') == '2023-12']\n","print(\"December 2023 rows:\", len(dec_23))\n","print(\"First timestamp in December 2023:\", dec_23['date'].iloc[0])\n","print(\"Last timestamp in December 2023:\", dec_23['date'].iloc[-1])\n"],"metadata":{"id":"pYhmKfmskwyZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# CONFIRM THE TOTAL ROWS\n","\n","# Calculate expected row count\n","start = pd.Timestamp(\"2012-01-01 00:15\")\n","end = pd.Timestamp(\"2023-12-31 00:00\")\n","expected_rows = int(((end - start).total_seconds() / 60) / 15) + 1\n","\n","print(\"Expected row count:\", expected_rows)\n","print(\"Actual row count:\", len(df))\n"],"metadata":{"id":"_Lf_THv_lF5B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# CONVERTING TO AN HOURLY DATA SET\n","\n","import pandas as pd\n","from datetime import datetime, timedelta\n","\n","# Step 1: Load the dataset without the 'date' column\n","file_path = \"/content/combined_time_series.csv\"\n","df = pd.read_csv(file_path)\n","df = df.drop(columns=['date'])  # Remove the date column\n","\n","# Step 2: Sum every 4 consecutive rows to create hourly data\n","# Reshape the data by summing over every 4 rows\n","hourly_df = df.groupby(df.index // 4).sum()\n","\n","# Step 3: Generate a new hourly date column\n","start_time = datetime(2012, 1, 1, 1, 0)  # Start at the first hour of Jan 1, 2012\n","hourly_dates = [start_time + timedelta(hours=i) for i in range(len(hourly_df))]\n","hourly_df['date'] = hourly_dates\n","\n","# Step 4: Save the resulting hourly dataset\n","output_file = \"/content/hourly_time_series.csv\"\n","hourly_df.to_csv(output_file, index=False)\n","\n","print(f\"Hourly dataset saved to {output_file}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PoJ0gvwiqJ6N","executionInfo":{"status":"ok","timestamp":1732912998993,"user_tz":360,"elapsed":2646,"user":{"displayName":"kushal bhalla","userId":"14624985245222236490"}},"outputId":"9565af86-994a-445c-a0a6-28d2f3556c58"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Hourly dataset saved to /content/hourly_time_series.csv\n"]}]},{"cell_type":"code","source":["# VERIFICATION 1 (start and end date)\n","\n","print(\"Start date:\", hourly_df['date'].iloc[0])\n","print(\"End date:\", hourly_df['date'].iloc[-1])\n"],"metadata":{"id":"lnkZBONCrzua"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# VERIFICATION 2 (number of rows in the dataset)\n","\n","print(\"Number of rows in hourly dataset:\", len(hourly_df))\n"],"metadata":{"id":"yshQMDmmr4Cb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# CREATING HOURLY DATA SETS\n","\n","import pandas as pd\n","\n","# Step 1: Load the hourly dataset\n","file_path = \"/content/hourly_time_series.csv\"\n","df = pd.read_csv(file_path)\n","\n","# Convert the 'date' column to datetime for filtering by time\n","df['date'] = pd.to_datetime(df['date'])\n","\n","# Step 2: Define the target hours\n","target_hours = [8, 14, 19, 0]  # 8 AM, 2 PM, 7 PM, 12 AM\n","\n","# Step 3: Filter the data for each target hour\n","filtered_datasets = {}\n","for hour in target_hours:\n","    filtered_df = df[df['date'].dt.hour == hour].reset_index(drop=True)\n","    filtered_datasets[hour] = filtered_df\n","\n","    # Save the filtered dataset\n","    output_file = f\"/content/daily_data_{hour:02d}.csv\"\n","    filtered_df.to_csv(output_file, index=False)\n","    print(f\"Dataset for {hour:02d}:00 saved to {output_file}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"udATknszyDVt","executionInfo":{"status":"ok","timestamp":1732915095963,"user_tz":360,"elapsed":638,"user":{"displayName":"kushal bhalla","userId":"14624985245222236490"}},"outputId":"7ae4b5a4-8ade-4d28-ad15-ebdcaffd55ef"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset for 08:00 saved to /content/daily_data_08.csv\n","Dataset for 14:00 saved to /content/daily_data_14.csv\n","Dataset for 19:00 saved to /content/daily_data_19.csv\n","Dataset for 00:00 saved to /content/daily_data_00.csv\n"]}]},{"cell_type":"code","source":["# VERIFICATION 1 checking the row count\n","\n","for hour, data in filtered_datasets.items():\n","    print(f\"Hour {hour:02d}: Number of rows = {len(data)}\")\n"],"metadata":{"id":"okTGlg350Jq5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# VERIFICATION 2 preview the data\n","\n","print(filtered_datasets[8].head())\n","print(filtered_datasets[8].tail())\n"],"metadata":{"id":"3CyclHXf0Opc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# VERIFICATION 3 checking the date column\n","\n","for hour, data in filtered_datasets.items():\n","    print(f\"Unique hours for {hour:02d} dataset:\", data['date'].dt.hour.unique())\n"],"metadata":{"id":"mwHEwUAb0cRM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ITXhe6740g6F"},"execution_count":null,"outputs":[]}]}